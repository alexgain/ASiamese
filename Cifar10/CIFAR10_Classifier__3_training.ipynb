{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Classifier. Training\n",
    "\n",
    "\n",
    "This is the third part of the tutorial on how to train a classifier on CIFAR10 dataset. \n",
    "\n",
    "Here we will assemble the results of two previous parts and train a model on CIFAR10.\n",
    "\n",
    "- Setup dataflow\n",
    "- Setup model: SqueezeNet v1.1\n",
    "- Setup loss function and optimizer\n",
    "- Setup training pipeline\n",
    "    - Training\n",
    "    - Inference\n",
    "\n",
    "References:\n",
    "- [pytorch-examples/imagenet](https://github.com/pytorch/examples/blob/master/imagenet/main.py)\n",
    "- [pytorch-transfer learning](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#sphx-glr-beginner-transfer-learning-tutorial-py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CIFAR10_ROOT'] = '/media/user/fast_storage/tensorpack_data/cifar10_data/'\n",
    "CIFAR10_ROOT = os.environ['CIFAR10_ROOT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataflow\n",
    "\n",
    "Again, we have two CIFAR10 datasets: `training` and `testing`. Next, we :\n",
    "- separate training dataset using stratified split into `n` folds of `training` and `validation` datasets.\n",
    "- apply data augmentations\n",
    "- gather data in batches\n",
    "- all previous operations using multiprocessing\n",
    "- load on GPU\n",
    "\n",
    "We won't iterate over folds and consider only the first fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Normalize, ToTensor, Lambda\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common_utils.dataflow import TransformedDataset, OnGPUDataLoader\n",
    "from common_utils.imgaug import ToNumpy, RandomOrder, RandomChoice, RandomFlip, RandomAffine, ColorJitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw datasets: training and testing\n",
    "train_ds = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=True, download=False)\n",
    "test_ds = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=False, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again resize input images to 42x42. Most of common state-of-the-art architectures requires input images larger than 224x224. Smaller input images will produce zero-size feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ResizeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, output_size=(32, 32)):        \n",
    "        assert isinstance(ds, Dataset)        \n",
    "        self.ds = ds\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.ds[index]\n",
    "        x = x.resize(self.output_size)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_train_ds = ResizeDataset(train_ds, output_size=(42, 42))\n",
    "resized_test_ds = ResizeDataset(test_ds, output_size=(42, 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_samples = len(resized_train_ds)\n",
    "X = np.zeros(n_samples)\n",
    "Y = np.zeros(n_samples)\n",
    "for i, (_, label) in enumerate(resized_train_ds):\n",
    "    Y[i] = label\n",
    "\n",
    "kfolds_train_indices = []\n",
    "kfolds_val_indices = []\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "for train_indices, val_indices in skf.split(X, Y):\n",
    "    kfolds_train_indices.append(train_indices)\n",
    "    kfolds_val_indices.append(val_indices)  \n",
    "    \n",
    "kfold_samplers = []\n",
    "for train_indices, val_indices in zip(kfolds_train_indices, kfolds_val_indices):\n",
    "    kfold_samplers.append({\"train\": SubsetRandomSampler(train_indices), \n",
    "                           \"val\": SubsetRandomSampler(val_indices)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentations :\n",
    "# following pytorch.org/docs/master/torchvision/models.html\n",
    "mean_val = [0.485, 0.456, 0.406]\n",
    "std_val = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Setup data augmentations\n",
    "train_transforms = Compose([\n",
    "    ToNumpy(),\n",
    "    # Geometry\n",
    "    RandomChoice([\n",
    "        RandomAffine(rotation=(-60, 60), scale=(0.95, 1.05), translate=(0.05, 0.05)),\n",
    "        RandomFlip(proba=0.5, mode='h'),\n",
    "        RandomFlip(proba=0.5, mode='v'),        \n",
    "    ]),    \n",
    "    # To Tensor (float, CxHxW, [0.0, 1.0]) + Normalize\n",
    "    ToTensor(),\n",
    "    # Color\n",
    "    ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    Normalize(mean_val, std_val)\n",
    "])\n",
    "  \n",
    "\n",
    "test_transforms = Compose([\n",
    "    ToNumpy(),    \n",
    "    # Geometry\n",
    "    RandomChoice([\n",
    "        RandomAffine(rotation=(-60, 60), scale=(0.95, 1.05), translate=(0.05, 0.05)),\n",
    "        RandomFlip(proba=0.5, mode='h'),\n",
    "        RandomFlip(proba=0.5, mode='v'),        \n",
    "    ]),        \n",
    "    # To Tensor (float, CxHxW, [0.0, 1.0])  + Normalize\n",
    "    ToTensor(),\n",
    "    # Color\n",
    "    ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15),\n",
    "    Normalize(mean_val, std_val)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug_train_ds = TransformedDataset(resized_train_ds, x_transforms=train_transforms)\n",
    "data_aug_val_ds = TransformedDataset(resized_train_ds, x_transforms=test_transforms)\n",
    "data_aug_test_ds = TransformedDataset(resized_test_ds, x_transforms=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = 0 \n",
    "\n",
    "_DataLoader = OnGPUDataLoader if torch.cuda.is_available() else DataLoader\n",
    "    \n",
    "    \n",
    "train_batches_ds = _DataLoader(data_aug_train_ds, \n",
    "                               batch_size=64, \n",
    "                               sampler=kfold_samplers[split_index][\"train\"], \n",
    "                               num_workers=0, \n",
    "                               drop_last=True, \n",
    "                               pin_memory=False)\n",
    "\n",
    "val_batches_ds = _DataLoader(data_aug_val_ds, \n",
    "                             batch_size=64, \n",
    "                             sampler=kfold_samplers[split_index][\"val\"], \n",
    "                             num_workers=0, \n",
    "                             drop_last=True, \n",
    "                             pin_memory=False)\n",
    "\n",
    "test_batches_ds = _DataLoader(data_aug_test_ds, \n",
    "                              batch_size=64, \n",
    "                              num_workers=0, \n",
    "                              drop_last=True, \n",
    "                              pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again training classes distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.43it/s, c0: 5 | c1: 3]]]\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "mean_n_classes = []\n",
    "std_n_classes = []\n",
    "cnt = 0\n",
    "\n",
    "classes_stats_per_batches = np.zeros((len(train_batches_ds), n_classes), dtype=np.int)\n",
    "\n",
    "pbar = tqdm(total=len(train_batches_ds))\n",
    "for i, (batch_x, batch_y) in enumerate(train_batches_ds):\n",
    "    for y in batch_y:\n",
    "        classes_stats_per_batches[i, y] += 1\n",
    "\n",
    "    postfix_str = \"c0: %i | c1: %i\" % (classes_stats_per_batches[i, 0], classes_stats_per_batches[i, 1])\n",
    "    pbar.set_postfix_str(postfix_str, refresh=True)\n",
    "    pbar.update(1)    \n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2cf4034390>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2cf452a7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data=classes_stats_per_batches, palette=\"PRGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2cf0433ef0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d853b0208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_n_classes = np.mean(np.cumsum(classes_stats_per_batches, axis=0), axis=1)\n",
    "std_n_classes = np.std(np.cumsum(classes_stats_per_batches, axis=0), axis=1)\n",
    "\n",
    "plt.plot(mean_n_classes)\n",
    "plt.plot(mean_n_classes + 3.0 * std_n_classes)\n",
    "plt.plot(mean_n_classes - 3.0 * std_n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model\n",
    "\n",
    "Let's load a small neural network \"SqueezeNet\" that has only ~700K parameters, showing performances similar to AlexNet:\n",
    "- Top-1 ImageNet Accuracy: 57.5% vs 57.2% (AlexNet)\n",
    "- Top-5 ImageNet Accuracy: 80.3% vs 80.3% (AlexNet)\n",
    "\n",
    "References:\n",
    "- [paper](https://arxiv.org/pdf/1602.07360.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import SqueezeNet\n",
    "\n",
    "from common_utils.nn_utils import print_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (5): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (8): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (12): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=13, stride=13, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "squeezenet = SqueezeNet(num_classes=10, version=1.1)\n",
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight torch.Size([64, 3, 3, 3])\n",
      "features.0.bias torch.Size([64])\n",
      "features.3.squeeze.weight torch.Size([16, 64, 1, 1])\n",
      "features.3.squeeze.bias torch.Size([16])\n",
      "features.3.expand1x1.weight torch.Size([64, 16, 1, 1])\n",
      "features.3.expand1x1.bias torch.Size([64])\n",
      "features.3.expand3x3.weight torch.Size([64, 16, 3, 3])\n",
      "features.3.expand3x3.bias torch.Size([64])\n",
      "features.4.squeeze.weight torch.Size([16, 128, 1, 1])\n",
      "features.4.squeeze.bias torch.Size([16])\n",
      "features.4.expand1x1.weight torch.Size([64, 16, 1, 1])\n",
      "features.4.expand1x1.bias torch.Size([64])\n",
      "features.4.expand3x3.weight torch.Size([64, 16, 3, 3])\n",
      "features.4.expand3x3.bias torch.Size([64])\n",
      "features.6.squeeze.weight torch.Size([32, 128, 1, 1])\n",
      "features.6.squeeze.bias torch.Size([32])\n",
      "features.6.expand1x1.weight torch.Size([128, 32, 1, 1])\n",
      "features.6.expand1x1.bias torch.Size([128])\n",
      "features.6.expand3x3.weight torch.Size([128, 32, 3, 3])\n",
      "features.6.expand3x3.bias torch.Size([128])\n",
      "features.7.squeeze.weight torch.Size([32, 256, 1, 1])\n",
      "features.7.squeeze.bias torch.Size([32])\n",
      "features.7.expand1x1.weight torch.Size([128, 32, 1, 1])\n",
      "features.7.expand1x1.bias torch.Size([128])\n",
      "features.7.expand3x3.weight torch.Size([128, 32, 3, 3])\n",
      "features.7.expand3x3.bias torch.Size([128])\n",
      "features.9.squeeze.weight torch.Size([48, 256, 1, 1])\n",
      "features.9.squeeze.bias torch.Size([48])\n",
      "features.9.expand1x1.weight torch.Size([192, 48, 1, 1])\n",
      "features.9.expand1x1.bias torch.Size([192])\n",
      "features.9.expand3x3.weight torch.Size([192, 48, 3, 3])\n",
      "features.9.expand3x3.bias torch.Size([192])\n",
      "features.10.squeeze.weight torch.Size([48, 384, 1, 1])\n",
      "features.10.squeeze.bias torch.Size([48])\n",
      "features.10.expand1x1.weight torch.Size([192, 48, 1, 1])\n",
      "features.10.expand1x1.bias torch.Size([192])\n",
      "features.10.expand3x3.weight torch.Size([192, 48, 3, 3])\n",
      "features.10.expand3x3.bias torch.Size([192])\n",
      "features.11.squeeze.weight torch.Size([64, 384, 1, 1])\n",
      "features.11.squeeze.bias torch.Size([64])\n",
      "features.11.expand1x1.weight torch.Size([256, 64, 1, 1])\n",
      "features.11.expand1x1.bias torch.Size([256])\n",
      "features.11.expand3x3.weight torch.Size([256, 64, 3, 3])\n",
      "features.11.expand3x3.bias torch.Size([256])\n",
      "features.12.squeeze.weight torch.Size([64, 512, 1, 1])\n",
      "features.12.squeeze.bias torch.Size([64])\n",
      "features.12.expand1x1.weight torch.Size([256, 64, 1, 1])\n",
      "features.12.expand1x1.bias torch.Size([256])\n",
      "features.12.expand3x3.weight torch.Size([256, 64, 3, 3])\n",
      "features.12.expand3x3.bias torch.Size([256])\n",
      "classifier.1.weight torch.Size([10, 512, 1, 1])\n",
      "classifier.1.bias torch.Size([10])\n",
      "\n",
      "Total number of trainable parameters:  727626\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(squeezenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the model on GPU if CUDA available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    squeezenet = squeezenet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we work with resized CIFAR10 images of size 42 x 42, we replace first layers that drastically reduce feature map size and adapt the last average pooling layer in order to avoid zero size feature maps. \n",
    "\n",
    "Let's see feature map sizes coming from `squeezenet.features`:\n",
    "- from the first layers before the first 'fire' module: \n",
    "    - 0: Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "    - 1: ReLU (inplace)\n",
    "    - 2: MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
    "- the last layer before classification part:\n",
    "    - 12: Fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AvgPool2d, Sequential, MaxPool2d, Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2)),\n",
       " MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezenet.features[0], squeezenet.features[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 23, 23]), torch.Size([1, 64, 11, 11]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 48, 48))\n",
    "if torch.cuda.is_available():\n",
    "    test_random_x.data = test_random_x.data.pin_memory()\n",
    "    test_random_x = test_random_x.cuda(async=True)\n",
    "    \n",
    "squeezenet.eval()\n",
    "test_output_y0 = squeezenet.features[0](test_random_x)\n",
    "test_output_y1 = Sequential(squeezenet.features[0], squeezenet.features[1], squeezenet.features[2])(test_random_x)\n",
    "test_output_y0.size(), test_output_y1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace first layers : \n",
    "- `Conv2d (3, 64, kernel_size=(3, 3), stride=(2, 2))` by `Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=1)`\n",
    "- remove `MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [l for i, l in enumerate(squeezenet.features) if i != 2]\n",
    "layers[0] = Conv2d(3, 64, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "squeezenet.features = Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (5): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (8): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=13, stride=13, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    squeezenet = squeezenet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 10, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 42, 42))\n",
    "if torch.cuda.is_available():\n",
    "    test_random_x.data = test_random_x.data.pin_memory()\n",
    "    test_random_x = test_random_x.cuda(async=True)\n",
    "\n",
    "squeezenet.eval()\n",
    "test_output_y = squeezenet.features(test_random_x)\n",
    "test_output_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AvgPool2d, Sequential\n",
    "\n",
    "layers = [l for l in squeezenet.classifier]\n",
    "layers[-1] = AvgPool2d(10)\n",
    "\n",
    "squeezenet.classifier = Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (5): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (8): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=10, stride=10, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from previous prints, the last layer of the model is average pooling and the output of the forward pass is logits and not yet probabilities of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0655  0.1968  0.0000  0.0957  0.0084  0.1635  0.2867  0.0758  0.0000  0.0000\n",
       "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 48, 48))\n",
    "if torch.cuda.is_available():\n",
    "    test_random_x.data = test_random_x.data.pin_memory()\n",
    "    test_random_x = test_random_x.cuda(async=True)\n",
    "\n",
    "squeezenet.eval()\n",
    "test_output_y = squeezenet(test_random_x)\n",
    "test_output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup loss function and optimizer\n",
    "\n",
    "Let's choose classical cross-entropy loss function for this multiclass classification task and Adam as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    criterion = CrossEntropyLoss().cuda()\n",
    "optimizer = Adam(squeezenet.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that CrossEntropyLoss:\n",
    "\n",
    "> This criterion combines `LogSoftMax` and `NLLLoss` in one single class.\n",
    "> It is useful when training a classification problem with `n` classes.\n",
    "\n",
    "> The `input` is expected to contain scores for each class.\n",
    "> `input` has to be a 2D `Tensor` of size `batch x n`.\n",
    "\n",
    "> This criterion expects a class index (0 to nClasses-1) as the\n",
    "> `target` for each value of a 1D tensor of size `n`\n",
    "\n",
    ">    The loss can be described as:\n",
    "$$\n",
    "        loss(x, class) = -log(exp(x[class]) / (\\sum_j exp(x[j])))\n",
    "                       = -x[class] + log(\\sum_j exp(x[j]))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check loss function computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.LongTensor'> <class 'torch.cuda.FloatTensor'> torch.Size([64]) torch.Size([64, 10])\n",
      "Loss :  \n",
      " 2.3020\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "squeezenet.eval()\n",
    "\n",
    "for i, (batch_x, batch_y) in enumerate(train_batches_ds):\n",
    "    \n",
    "    batch_x = Variable(batch_x, requires_grad=True)\n",
    "    batch_y = Variable(batch_y)\n",
    "    batch_y_pred = squeezenet(batch_x)\n",
    "    print(type(batch_y.data), type(batch_y_pred.data), batch_y.size(), batch_y_pred.size())\n",
    "    loss = criterion(batch_y_pred, batch_y)\n",
    "    print(\"Loss : \", loss.data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training pipeline\n",
    "\n",
    "Typical pipeline: \n",
    "- loops over training dataset during `n_epochs`\n",
    "    - computes values of the loss function, accuracy, other metrics on train batches\n",
    "- runs validation phase on validation dataset when training epoch ends\n",
    "    - computes values of the loss function, accuracy, other metrics on whole validation dataset\n",
    "- save on the disk the best model defined by a metric\n",
    "- performs learning rate scheduling on each training epoch\n",
    "\n",
    "\n",
    "Next, there are two ways:\n",
    "- copy/modify code from examples: i.e. [link](https://github.com/pytorch/examples/blob/master/imagenet/main.py)\n",
    "- use [torchsample](https://github.com/ncullen93/torchsample) or [tnt](https://github.com/pytorch/tnt) ...\n",
    "\n",
    "Here we choose the first way and explicitly code a training pipeline based on various available examples.\n",
    "Let's first define methods called in the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.training_utils import AverageMeter, accuracy, save_checkpoint\n",
    "from common_utils.training_utils import train_one_epoch, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_epoch??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0 \n",
    "n_epochs = 10\n",
    "model = squeezenet\n",
    "init_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.backends import cudnn\n",
    "cudnn.benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What does torch.backends.cudnn.benchmark do? [url](https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(squeezenet.parameters(), lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10: 100%|##########| 625/625 [00:24<00:00, 25.64it/s, Loss 1.9555 | accuracy 25.645]\n",
      "100%|##########| 156/156 [00:04<00:00, 36.99it/s, Loss 1.7940 | accuracy 33.814]\n",
      "Epoch: 2/10: 100%|##########| 625/625 [00:24<00:00, 26.01it/s, Loss 1.7237 | accuracy 36.190]\n",
      "100%|##########| 156/156 [00:04<00:00, 37.31it/s, Loss 1.6417 | accuracy 40.014]\n",
      "Epoch: 3/10: 100%|##########| 625/625 [00:23<00:00, 26.09it/s, Loss 1.6216 | accuracy 40.570]\n",
      "100%|##########| 156/156 [00:04<00:00, 37.04it/s, Loss 1.6617 | accuracy 39.093]\n",
      "Epoch: 4/10: 100%|##########| 625/625 [00:23<00:00, 26.11it/s, Loss 1.5548 | accuracy 43.110]\n",
      "100%|##########| 156/156 [00:04<00:00, 37.17it/s, Loss 1.5360 | accuracy 44.081]\n",
      "Epoch: 5/10: 100%|##########| 625/625 [00:23<00:00, 26.36it/s, Loss 1.4966 | accuracy 45.535]\n",
      "100%|##########| 156/156 [00:04<00:00, 37.22it/s, Loss 1.5705 | accuracy 43.279]\n",
      "Epoch: 6/10: 100%|##########| 625/625 [00:23<00:00, 26.25it/s, Loss 1.4552 | accuracy 47.265]\n",
      "100%|##########| 156/156 [00:04<00:00, 37.19it/s, Loss 1.4185 | accuracy 49.018]\n",
      "Epoch: 7/10: 100%|##########| 625/625 [00:23<00:00, 26.20it/s, Loss 1.4064 | accuracy 48.898]\n",
      "100%|##########| 156/156 [00:04<00:00, 37.17it/s, Loss 1.3953 | accuracy 49.559]\n",
      "Epoch: 8/10: 100%|##########| 625/625 [00:23<00:00, 26.18it/s, Loss 1.3777 | accuracy 50.255]\n",
      "100%|##########| 156/156 [00:04<00:00, 37.24it/s, Loss 1.3145 | accuracy 52.815]\n",
      "Epoch: 9/10: 100%|##########| 625/625 [00:23<00:00, 26.43it/s, Loss 1.3419 | accuracy 51.413]\n",
      "100%|##########| 156/156 [00:04<00:00, 37.17it/s, Loss 1.3736 | accuracy 50.541]\n",
      "Epoch: 10/10: 100%|##########| 625/625 [00:23<00:00, 26.29it/s, Loss 1.3133 | accuracy 52.575]\n",
      "100%|##########| 156/156 [00:04<00:00, 37.36it/s, Loss 1.3058 | accuracy 53.245]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "now = datetime.now()\n",
    "\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "    \n",
    "logs_path = os.path.join('logs', 'cifar10_squeezenet_%s' % now.strftime(\"%Y%m%d_%H%M\"))\n",
    "if not os.path.exists(logs_path):\n",
    "    os.makedirs(logs_path)    \n",
    "\n",
    "    \n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    # train for one epoch\n",
    "    ret = train_one_epoch(model, train_batches_ds, criterion, optimizer, epoch, n_epochs, avg_metrics=[accuracy, ])\n",
    "    if ret is None:\n",
    "        break\n",
    "    loss, acc = ret\n",
    "\n",
    "    # evaluate on validation set\n",
    "    ret = validate(model, val_batches_ds, criterion, avg_metrics=[accuracy, ])\n",
    "    if ret is None:\n",
    "        break\n",
    "    val_loss, val_acc = ret\n",
    "\n",
    "    # remember best accuracy and save checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = max(val_acc, best_acc)\n",
    "        save_checkpoint(logs_path, 'val_acc', {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'optimizer' : optimizer.state_dict()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Let's upload the best saved model and run inference on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.training_utils import load_checkpoint\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_filenames = glob(os.path.join(logs_path, \"model_val_acc=*\"))\n",
    "assert len(saved_model_filenames) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint: logs/cifar10_squeezenet_20171122_0756/model_val_acc=62.0092.pth.tar\n"
     ]
    }
   ],
   "source": [
    "saved_model_filename = saved_model_filenames[0]\n",
    "\n",
    "load_checkpoint(saved_model_filename, squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 156/156 [00:04<00:00, 36.58it/s, Loss 1.0840 | accuracy 62.470]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62.46995192307692"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on validation set\n",
    "test_loss, test_acc = validate(squeezenet, test_batches_ds, criterion, avg_metrics=[accuracy, ])\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
