{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese neural networks training\n",
    "\n",
    "This notebook presents the paper [\"Siamese Neural Networks for One-shot Image Recognition\"](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) coded with PyTorch framework. \n",
    "\n",
    "In this part we train Siamese network on the Omniglot dataset to perform the classification task to distinguish two images of the same class or different classes.\n",
    "\n",
    "\n",
    "References:\n",
    "- [paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "- [omniglot](https://github.com/brendenlake/omniglot)\n",
    "- [keras-oneshot](https://github.com/sorenbouma/keras-oneshot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAS_GPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataflow import OmniglotDataset, SameOrDifferentPairsDataset, PairTransformedDataset\n",
    "from common_utils.imgaug import RandomAffine, RandomApply\n",
    "from common_utils.dataflow import TransformedDataset, OnGPUDataLoader, ResizedDataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "OMNIGLOT_REPO_PATH='omniglot'\n",
    "\n",
    "TRAIN_DATA_PATH = os.path.join(OMNIGLOT_REPO_PATH, 'python', 'images_background')\n",
    "train_alphabets = !ls {TRAIN_DATA_PATH}\n",
    "train_alphabets = list(train_alphabets)\n",
    "\n",
    "TEST_DATA_PATH = os.path.join(OMNIGLOT_REPO_PATH, 'python', 'images_evaluation')\n",
    "test_alphabets = !ls {TEST_DATA_PATH}\n",
    "test_alphabets = list(test_alphabets)\n",
    "\n",
    "assert len(train_alphabets) > 1 and len(test_alphabets) > 1, \"%s \\n %s\" % (train_alphabets[0], test_alphabets[0])\n",
    "\n",
    "train_alphabet_char_id_drawer_ids = {}\n",
    "for a in train_alphabets:\n",
    "    res = !ls \"{os.path.join(TRAIN_DATA_PATH, a)}\"\n",
    "    char_ids = list(res)\n",
    "    train_alphabet_char_id_drawer_ids[a] = {}\n",
    "    for char_id in char_ids:\n",
    "        res = !ls \"{os.path.join(TRAIN_DATA_PATH, a, char_id)}\"\n",
    "        train_alphabet_char_id_drawer_ids[a][char_id] = [_id[:-4] for _id in list(res)]\n",
    "        \n",
    "        \n",
    "test_alphabet_char_id_drawer_ids = {}\n",
    "for a in test_alphabets:\n",
    "    res = !ls \"{os.path.join(TEST_DATA_PATH, a)}\"\n",
    "    char_ids = list(res)\n",
    "    test_alphabet_char_id_drawer_ids[a] = {}\n",
    "    for char_id in char_ids:\n",
    "        res = !ls \"{os.path.join(TEST_DATA_PATH, a, char_id)}\"\n",
    "        test_alphabet_char_id_drawer_ids[a][char_id] = [_id[:-4] for _id in list(res)]\n",
    "\n",
    "\n",
    "# Sample 12 drawers out of 20\n",
    "all_drawers_ids = np.arange(20) \n",
    "# train_drawers_ids = np.random.choice(all_drawers_ids, size=12, replace=False)\n",
    "train_drawers_ids = np.arange(20)\n",
    "# Sample 4 drawers out of remaining 8\n",
    "val_drawers_ids = np.random.choice(list(set(all_drawers_ids) - set(train_drawers_ids)), size=4, replace=False)\n",
    "test_drawers_ids = np.array(list(set(all_drawers_ids) - set(val_drawers_ids) - set(train_drawers_ids)))\n",
    "\n",
    "def create_str_drawers_ids(drawers_ids):\n",
    "    return [\"_{0:0>2}\".format(_id) for _id in drawers_ids]\n",
    "\n",
    "train_drawers_ids = create_str_drawers_ids(train_drawers_ids)\n",
    "val_drawers_ids = create_str_drawers_ids(val_drawers_ids)\n",
    "test_drawers_ids = create_str_drawers_ids(test_drawers_ids)\n",
    "\n",
    "train_ds = OmniglotDataset(\"Train\", data_path=TRAIN_DATA_PATH, \n",
    "                           alphabet_char_id_drawers_ids=train_alphabet_char_id_drawer_ids, \n",
    "                           drawers_ids=train_drawers_ids)\n",
    "\n",
    "val_ds = OmniglotDataset(\"Test\", data_path=TEST_DATA_PATH, \n",
    "                         alphabet_char_id_drawers_ids=test_alphabet_char_id_drawer_ids, \n",
    "                         drawers_ids=val_drawers_ids)\n",
    "\n",
    "test_ds = OmniglotDataset(\"Test\", data_path=TEST_DATA_PATH, \n",
    "                          alphabet_char_id_drawers_ids=test_alphabet_char_id_drawer_ids, \n",
    "                          drawers_ids=test_drawers_ids)\n",
    "\n",
    "train_ds = ResizedDataset(train_ds, output_size=(28, 28))\n",
    "val_ds = ResizedDataset(val_ds, output_size=(28, 28))\n",
    "test_ds = ResizedDataset(test_ds, output_size=(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 10000, 10000)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs = SameOrDifferentPairsDataset(train_ds, nb_pairs=int(30e3))\n",
    "val_pairs = SameOrDifferentPairsDataset(val_ds, nb_pairs=int(10e3))\n",
    "test_pairs = SameOrDifferentPairsDataset(test_ds, nb_pairs=int(10e3))\n",
    "\n",
    "len(train_pairs), len(val_pairs), len(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_aug = Compose([\n",
    "    RandomApply(\n",
    "        RandomAffine(rotation=(-10, 10), scale=(0.8, 1.2), translate=(-0.05, 0.05)),\n",
    "        proba=0.5\n",
    "    ),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "test_data_aug = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "y_transform = lambda y: torch.FloatTensor([y])\n",
    "\n",
    "train_aug_pairs = PairTransformedDataset(train_pairs, x_transforms=train_data_aug, y_transforms=y_transform)\n",
    "val_aug_pairs = PairTransformedDataset(val_pairs, x_transforms=test_data_aug, y_transforms=y_transform)\n",
    "test_aug_pairs = PairTransformedDataset(test_pairs, x_transforms=test_data_aug, y_transforms=y_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 156, 157)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "_DataLoader = OnGPUDataLoader if HAS_GPU and torch.cuda.is_available() else DataLoader\n",
    "\n",
    "train_batches = _DataLoader(train_aug_pairs, batch_size=batch_size, \n",
    "                            shuffle=True, num_workers=12, \n",
    "                            drop_last=True)\n",
    "\n",
    "val_batches = _DataLoader(val_aug_pairs, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=12,\n",
    "                          pin_memory=True, drop_last=True)\n",
    "\n",
    "test_batches = _DataLoader(test_aug_pairs, batch_size=batch_size, \n",
    "                           shuffle=False, num_workers=12,                   \n",
    "                           pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "len(train_batches), len(val_batches), len(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64, 1, 28, 28]) torch.Size([64, 1])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for (x1, x2), y in train_batches:\n",
    "    print(x1.size(), x2.size(), y.size())\n",
    "    print(type(x1), type(x1), type(y))    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model, loss function and optimisation algorithm\n",
    "\n",
    "#### Weight regularization\n",
    "\n",
    "L2 weights regularization: \n",
    "\n",
    "#### Loss function\n",
    "\n",
    "Binary cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.nn.functional import sigmoid\n",
    "from torch.optim import Adam, RMSprop, SGD\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from common_utils.training_utils import train_one_epoch, validate, write_csv_log, write_conf_log, verbose_optimizer, save_checkpoint\n",
    "from common_utils.training_utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SiameseNetworks, ATwoLayer, OmniV, Classifier2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net = Classifier2(image_size=28*2, tasks = 1)\n",
    "# siamese_net = OmniV(image_size=28, tasks = 1)\n",
    "# siamese_net = ATwoLayer(input_size=105*105*2, output = 1, tasks = 1)\n",
    "# siamese_net = SiameseNetworks(input_shape=(105, 105, 1))\n",
    "if HAS_GPU and torch.cuda.is_available():\n",
    "    siamese_net = siamese_net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_logits(y_logits, y_true):\n",
    "    y_pred = sigmoid(y_logits).data\n",
    "    return accuracy(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BCEWithLogitsLoss()\n",
    "if HAS_GPU and torch.cuda.is_available():\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 1, 1])\n",
      "torch.Size([64, 64])\n",
      "DatasetError: size mismatch, m1: [64 x 64], m2: [576 x 1] at ../aten/src/TH/generic/THTensorMath.cpp:961\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sigmoid(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-e439e604813a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     batch_y_logits = siamese_net.forward(batch_x1, batch_x2,task=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_x1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_x2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mbatch_y_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/Research_School/Spring 2019/multi-modality/continual_learning/SiameseNetworks/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_input, task)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sigmoid(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "# Test single forward pass and loss function computation\n",
    "siamese_net.eval()\n",
    "for i, ((batch_x1, batch_x2), batch_y) in enumerate(train_batches):\n",
    "    \n",
    "    batch_x1 = Variable(batch_x1, requires_grad=True)\n",
    "    batch_x2 = Variable(batch_x2, requires_grad=True)    \n",
    "    batch_y = Variable(batch_y)\n",
    "    \n",
    "#     batch_y_logits = siamese_net(batch_x1, batch_x2)\n",
    "#     batch_y_logits = siamese_net.forward(batch_x1, batch_x2,task=0)\n",
    "    batch_x = torch.cat([batch_x1,batch_x2],dim=1)\n",
    "    print(batch_x.shape)\n",
    "    batch_y_logits = siamese_net.forward(batch_x, task=0)\n",
    "\n",
    "\n",
    "    print(type(batch_y.data), type(batch_y_logits.data), batch_y.size(), batch_y_logits.size())    \n",
    "    loss = criterion(batch_y_logits, batch_y)\n",
    "    print(\"Loss : \", loss.data)\n",
    "    \n",
    "    print(\"Accuracy : \", accuracy_logits(batch_y_logits.data, batch_y.data))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 56, 28])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(64,28,28)\n",
    "x2 = torch.randn(64,28,28)\n",
    "print(torch.cat([x1,x2],dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam([{\n",
    "#     'params': siamese_net.net.features.parameters(),\n",
    "#     'lr': conf['lr_features'],    \n",
    "# }, {\n",
    "#     'params': siamese_net.classifier.parameters(),\n",
    "#     'lr': conf['lr_classifier']\n",
    "# }],\n",
    "#     weight_decay=conf['weight_decay']\n",
    "# )\n",
    "\n",
    "optimizer = Adam(siamese_net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we define L2 regularization weights through optimizer API as `weight_decay` parameter, [ref](http://pytorch.org/docs/master/optim.html?highlight=adam#torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "logs_path = os.path.join('logs', 'seamese_networks_verification_task_%s' % (now.strftime(\"%Y%m%d_%H%M\")))\n",
    "if not os.path.exists(logs_path):\n",
    "    os.makedirs(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizer: Adam\n",
      "- Param group: \n",
      "\tlr: 1e-05\n",
      "\tbetas: (0.9, 0.999)\n",
      "\teps: 1e-08\n",
      "\tweight_decay: 0\n",
      "\tamsgrad: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50:  95%|#########5| 446/468 [04:23<00:11,  1.83it/s, Loss 0.6931 | accuracy_logits 0.500]Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from common_utils.training_utils import train_one_epoch, validate, write_csv_log, write_conf_log, verbose_optimizer, save_checkpoint\n",
    "write_conf_log(logs_path, \"{}\".format(conf))\n",
    "write_conf_log(logs_path, verbose_optimizer(optimizer))\n",
    "\n",
    "write_csv_log(logs_path, \"epoch,train_loss,train_acc,val_loss,val_acc\")\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(conf['n_epochs']):\n",
    "#     scheduler.step()\n",
    "    # Verbose learning rates:\n",
    "    print(verbose_optimizer(optimizer))\n",
    "\n",
    "    # train for one epoch\n",
    "    ret = train_one_epoch(siamese_net, train_batches, \n",
    "                          criterion, optimizer,                                               \n",
    "                          epoch, conf['n_epochs'], avg_metrics=[accuracy_logits,])\n",
    "    if ret is None:\n",
    "        break\n",
    "    train_loss, train_acc = ret\n",
    "\n",
    "    # evaluate on validation set\n",
    "    ret = validate(siamese_net, val_batches, criterion, avg_metrics=[accuracy_logits, ])\n",
    "    if ret is None:\n",
    "        break\n",
    "    val_loss, val_acc = ret\n",
    "    \n",
    "#     onplateau_scheduler.step(val_loss)\n",
    "\n",
    "    # Write a csv log file\n",
    "    write_csv_log(logs_path, \"%i,%f,%f,%f,%f\" % (epoch, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "    # remember best accuracy and save checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_prec1 = max(val_acc, best_acc)\n",
    "        save_checkpoint(logs_path, 'val_acc', \n",
    "                        {'epoch': epoch + 1,\n",
    "                         'state_dict': siamese_net.state_dict(),\n",
    "                         'val_acc': val_acc,           \n",
    "                         'optimizer': optimizer.state_dict()})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.training_utils import load_checkpoint\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cf873a22fa55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_model_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_val_acc=*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_filenames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_filenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msiamese_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model_filenames = glob(os.path.join(logs_path, \"model_val_acc=*\"))\n",
    "assert len(best_model_filenames) == 1\n",
    "load_checkpoint(best_model_filenames[0], siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/157 [00:00<?, ?it/s]../common_utils/training_utils.py:102: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  batch_x = [Variable(batch_, volatile=True) for batch_ in batch_x]\n",
      "../common_utils/training_utils.py:105: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  batch_y = Variable(batch_y, volatile=True)\n",
      "100%|##########| 157/157 [1:39:29<00:00,  3.76s/it, Loss 0.6924 | accuracy_logits 0.500]     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.692438431930542, tensor(0.5000))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on validation set\n",
    "test_loss, test_acc = validate(siamese_net, test_batches, criterion, avg_metrics=[accuracy_logits, ])\n",
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
